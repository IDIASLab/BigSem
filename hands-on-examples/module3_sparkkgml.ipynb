{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a357468",
      "metadata": {
        "id": "4a357468"
      },
      "source": [
        "# SparkKG-ML Tutorial\n",
        "\n",
        "Welcome to this tutorial on **SparkKG-ML**, a Python library designed to facilitate machine learning with Spark on semantic web and knowledge graph data.\n",
        "\n",
        "In this notebook, we will walk through the installation of SparkKG-ML, and demonstrate some of the key functionalities including data acquisition from SPARQL endpoints, feature engineering, vectorization, and semantification. We will also create a simple pipeline and evaluate the results.\n",
        "\n",
        "## Installation\n",
        "\n",
        "We begin by installing the SparkKG-ML library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a154006b",
      "metadata": {
        "id": "a154006b"
      },
      "outputs": [],
      "source": [
        "!pip install sparkkgml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db8b0ef3",
      "metadata": {
        "id": "db8b0ef3"
      },
      "source": [
        "## Data Acquisition\n",
        "\n",
        "We will retrieve data from out ttl file and convert it into a Spark DataFrame using the `getDataFrame` function. Here's how you can achieve that.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparkkgml.data_acquisition import DataAcquisition\n",
        "\n",
        "# Create an instance of DataAcquisition\n",
        "dataAcquisitionObject = DataAcquisition()\n",
        "\n",
        "query ='''\n",
        "    PREFIX schema: <https://schema.org/>\n",
        "    PREFIX recipeKG:<http://purl.org/recipekg/>\n",
        "    SELECT  ?recipe\n",
        "    WHERE { ?recipe a schema:Recipe. }\n",
        "    LIMIT 3\n",
        "'''\n",
        "spark_df = dataAcquisitionObject.query_local_rdf(\"recipekg_100.ttl\",'ttl', query)\n",
        "spark_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "POFX3PPs-zld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c2e7119-e48e-4636-83dd-aeb1b273afdc"
      },
      "id": "POFX3PPs-zld",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drop the columns where at least %0 element is missing.\n",
            "Drop the rows where at least %100 element is missing.\n",
            "+--------------------------------------------------------+\n",
            "|recipe                                                  |\n",
            "+--------------------------------------------------------+\n",
            "|http://purl.org/recipekg/recipe/peanut-butter-tandy-bars|\n",
            "|http://purl.org/recipekg/recipe/the-best-oatmeal-cookies|\n",
            "|http://purl.org/recipekg/recipe/peach-cobbler-ii        |\n",
            "+--------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3646b987",
      "metadata": {
        "id": "3646b987"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Let's get more information about the recipes.\n",
        "After acquiring the data, we can use the `getFeatures` function to extract features and their descriptions from the Spark DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "# Specify the SPARQL endpoint and query\n",
        "query2 =\"\"\"\n",
        "     PREFIX schema: <https://schema.org/>\n",
        "     PREFIX recipeKG:<http://purl.org/recipekg/>\n",
        "     PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "     SELECT DISTINCT ?recipe ?calorie ?category ?USDAScore\n",
        "     WHERE {\n",
        "                 ?recipe a schema:Recipe.\n",
        "\n",
        "                 ?recipe recipeKG:hasNutritionalInformation ?a.\n",
        "                 ?a recipeKG:hasCalorificData ?b.\n",
        "                 ?b recipeKG:hasAmount ?calorie.\n",
        "\n",
        "                 ?recipe recipeKG:belongsTo ?subcategory.\n",
        "                 ?subcategory rdfs:subClassOf* ?category.\n",
        "                 ?category a recipeKG:RecipeCategory.\n",
        "\n",
        "                 ?recipe recipeKG:hasUSDAScore ?USDAScore.\n",
        "         }\n",
        "         LIMIT 10\n",
        "     \"\"\"\n",
        "\n",
        "# Retrieve the data as a Spark DataFrame\n",
        "spark_df = dataAcquisitionObject.query_local_rdf(\"recipekg_100.ttl\",'turtle', query2)\n",
        "#let's also delete the url and just have names\n",
        "spark_df = spark_df.withColumn(\"recipe\", regexp_replace('recipe','http://purl.org/recipekg/recipe/',''))\n",
        "spark_df = spark_df.withColumn(\"category\", regexp_replace('category','http://purl.org/recipekg/categories/',''))\n",
        "spark_df = spark_df.withColumn(\"category\", regexp_replace('category','/',''))\n",
        "spark_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPSWhqCmDTpn",
        "outputId": "19139c7b-70b8-4bc6-a46b-942aa94b1254"
      },
      "id": "aPSWhqCmDTpn",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drop the columns where at least %0 element is missing.\n",
            "Drop the rows where at least %100 element is missing.\n",
            "+--------------------------------------------+-------+-------------------------------+---------+\n",
            "|recipe                                      |calorie|category                       |USDAScore|\n",
            "+--------------------------------------------+-------+-------------------------------+---------+\n",
            "|peanut-butter-tandy-bars                    |230.0  |desserts                       |3        |\n",
            "|the-best-oatmeal-cookies                    |172.8  |desserts                       |4        |\n",
            "|peach-cobbler-ii                            |672.4  |desserts                       |1        |\n",
            "|pie-crust-v                                 |210.4  |desserts                       |3        |\n",
            "|dads-beef-and-chive-dip                     |77.6   |appetizers-and-snacks          |4        |\n",
            "|palak-paneer-indian-spinach-and-paneer      |315.1  |trusted-brands-recipes-and-tips|4        |\n",
            "|corn-and-porcini-mushroom-cornbread-dressing|214.7  |side-dish                      |3        |\n",
            "|easy-mexican-fried-chicken                  |548.1  |meat-and-poultry               |3        |\n",
            "|orange-raisin-cake                          |168.2  |desserts                       |4        |\n",
            "|onion-masala-omelette                       |522.3  |breakfast-and-brunch           |4        |\n",
            "+--------------------------------------------+-------+-------------------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "3b009366",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b009366",
        "outputId": "bac0b594-c921-405c-8226-0c05c4ae3ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No entity column has been set, that is why the first column recipe is used as entity column\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'calorie': {'featureType': 'Single_NonCategorical_Double',\n",
              "  'name': 'calorie',\n",
              "  'nullable': False,\n",
              "  'datatype': DoubleType(),\n",
              "  'numberDistinctValues': 10,\n",
              "  'isListOfEntries': False,\n",
              "  'isCategorical': False},\n",
              " 'category': {'featureType': 'Single_NonCategorical_String',\n",
              "  'name': 'category',\n",
              "  'nullable': False,\n",
              "  'datatype': StringType(),\n",
              "  'numberDistinctValues': 6,\n",
              "  'isListOfEntries': False,\n",
              "  'isCategorical': False},\n",
              " 'USDAScore': {'featureType': 'Single_NonCategorical_Long',\n",
              "  'name': 'USDAScore',\n",
              "  'nullable': False,\n",
              "  'datatype': LongType(),\n",
              "  'numberDistinctValues': 3,\n",
              "  'isListOfEntries': False,\n",
              "  'isCategorical': False}}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "from sparkkgml.feature_engineering import FeatureEngineering\n",
        "\n",
        "# Create an instance of FeatureEngineering\n",
        "featureEngineeringObject = FeatureEngineering()\n",
        "\n",
        "# Extract features\n",
        "df2, features = featureEngineeringObject.getFeatures(spark_df)\n",
        "features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11afebaf",
      "metadata": {
        "id": "11afebaf"
      },
      "source": [
        "## Vectorization\n",
        "\n",
        "Next, we can vectorize the features we extracted using the `vectorize` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "36c188dd",
      "metadata": {
        "id": "36c188dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab32ba6-40ec-4d49-a262-b532d01050bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No entity column has been set, that is why the first column recipe is used as entity column\n",
            "+--------------------+-------+--------------------+---------+\n",
            "|              recipe|calorie|            category|USDAScore|\n",
            "+--------------------+-------+--------------------+---------+\n",
            "|corn-and-porcini-...|  214.7|[0.06451535224914...|        3|\n",
            "|dads-beef-and-chi...|   77.6|[-0.1662092804908...|        4|\n",
            "|easy-mexican-frie...|  548.1|[0.06132638454437...|        3|\n",
            "|onion-masala-omel...|  522.3|[-0.0420703291893...|        4|\n",
            "|  orange-raisin-cake|  168.2|[0.11072149872779...|        4|\n",
            "+--------------------+-------+--------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparkkgml.vectorization import Vectorization\n",
        "\n",
        "# Create an instance of Vectorization\n",
        "vectorizationObject = Vectorization()\n",
        "\n",
        "# Vectorize the DataFrame\n",
        "digitized_df = vectorizationObject.vectorize(df2, features)\n",
        "digitized_df.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Machine Learning Task with Spark MLlib: Predicting USDA Scores\n",
        "\n",
        "In this section, we will use Spark's MLlib library to predict USDA scores directly from `digitized_df`, which already has vectorized features for `coterie` and `category`.\n",
        "\n",
        "### Steps\n",
        "1. Assemble the features using VectorAssembler.\n",
        "2. Split data into training and test sets.\n",
        "3. Train a machine learning model using RandomForestRegressor from Spark MLlib.\n",
        "4. Evaluate the model's performance.\n"
      ],
      "metadata": {
        "id": "VZu-PDl4HAbh"
      },
      "id": "VZu-PDl4HAbh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Feature Assembly with VectorAssembler\n",
        "# Assuming `digitized_df` has the required 'coterie', 'category', and 'USDA_score' columns\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Assemble features into a single vector column for Spark MLlib\n",
        "assembler = VectorAssembler(inputCols=['calorie', 'category'], outputCol='features')\n",
        "df_with_features = assembler.transform(digitized_df).select('recipe','features', 'USDAScore')\n",
        "df_with_features.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-efeQ_EFe2c",
        "outputId": "a9d3b912-18ef-4f46-9629-459692169492"
      },
      "id": "H-efeQ_EFe2c",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---------+\n",
            "|              recipe|            features|USDAScore|\n",
            "+--------------------+--------------------+---------+\n",
            "|corn-and-porcini-...|[214.7,0.06451535...|        3|\n",
            "|dads-beef-and-chi...|[77.6,-0.16620928...|        4|\n",
            "|easy-mexican-frie...|[548.1,0.06132638...|        3|\n",
            "|onion-masala-omel...|[522.3,-0.0420703...|        4|\n",
            "|  orange-raisin-cake|[168.2,0.11072149...|        4|\n",
            "|palak-paneer-indi...|[315.1,-0.0512769...|        4|\n",
            "|    peach-cobbler-ii|[672.4,0.11072149...|        1|\n",
            "|peanut-butter-tan...|[230.0,0.11072149...|        3|\n",
            "|         pie-crust-v|[210.4,0.11072149...|        3|\n",
            "|the-best-oatmeal-...|[172.8,0.11072149...|        4|\n",
            "+--------------------+--------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Split data into training and testing sets\n",
        "train_df, test_df = df_with_features.randomSplit([0.9, 0.1], seed=42)"
      ],
      "metadata": {
        "id": "p4GJQWS0HXMi"
      },
      "id": "p4GJQWS0HXMi",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "\n",
        "# Step 3: Model Training using Spark MLlib's RandomForestRegressor\n",
        "# Initialize RandomForestRegressor from Spark MLlib\n",
        "rf = RandomForestRegressor(featuresCol='features', labelCol='USDAScore', numTrees=10, maxDepth=10, seed=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model = rf.fit(train_df)"
      ],
      "metadata": {
        "id": "LfugjX3YHKS6"
      },
      "id": "LfugjX3YHKS6",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Model Evaluation\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = rf_model.transform(test_df)\n",
        "predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW8DLrqgHQYe",
        "outputId": "eba3867d-9448-4d04-9375-006bffd2031f"
      },
      "id": "vW8DLrqgHQYe",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---------+------------------+\n",
            "|              recipe|            features|USDAScore|        prediction|\n",
            "+--------------------+--------------------+---------+------------------+\n",
            "|easy-mexican-frie...|[548.1,0.06132638...|        3|3.5799999999999996|\n",
            "|    peach-cobbler-ii|[672.4,0.11072149...|        1|              3.63|\n",
            "|         pie-crust-v|[210.4,0.11072149...|        3|3.5966666666666667|\n",
            "+--------------------+--------------------+---------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "251e250b",
      "metadata": {
        "id": "251e250b"
      },
      "source": [
        "## Semantification\n",
        "\n",
        "Finally, we will use the `semantify` function to convert the DataFrame results into RDF data in Turtle format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "9ac69b8f",
      "metadata": {
        "id": "9ac69b8f"
      },
      "outputs": [],
      "source": [
        "from sparkkgml.semantification import Semantification\n",
        "\n",
        "# Create an instance of Semantification\n",
        "semantificationObject = Semantification()\n",
        "\n",
        "# Semantify the data\n",
        "semantificationObject.semantify(predictions, namespace=\"http://purl.org/recipekg/recipe/\", exp_uri=\"recipe\", exp_label=\"USDAScore\", exp_prediction=\"prediction\", dest=\"output.ttl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd8d018e",
      "metadata": {
        "id": "fd8d018e"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we installed SparkKG-ML, retrieved data from a SPARQL endpoint, performed feature engineering, vectorized the data, and finally semantified the machine learning results. This demonstrates how SparkKG-ML facilitates a complete machine learning pipeline with semantic web and knowledge graph data.\n",
        "\n",
        "Feel free to explore the additional functionalities in the SparkKG-ML documentation.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}